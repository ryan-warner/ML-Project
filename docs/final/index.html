<!DOCTYPE html><html lang="en"> <head><meta charset="utf-8"><meta name="description" content="Astro description"><meta name="viewport" content="width=device-width"><link rel="icon" type="image/svg+xml" href="favicon.svg"><meta name="generator" content="Astro v4.4.4"><link rel="stylesheet" href="/pages/rwarner31/ML-Project/style.css"><title>ML Project</title><link rel="stylesheet" href="/pages/rwarner31/ML-Project/_astro/contributors.DjCMD17D.css" /></head> <body class="bg-light-primary h-screen flex flex-col justify-between"> <div> <div class="h-18 w-full flex justify-between py-6 px-12"> <div class="text-black flex items-center justify-center"> <a href="/pages/rwarner31/ML-Project/" class="text-4xl font-bold">CS 4641 Project</a> </div> <div class="flex gap-4"> <a class="flex flex-col justify-center" href="/pages/rwarner31/ML-Project/final"> <div class="text-black text-2xl hover:opacity-80">Final Report</div> </a> <a class="flex flex-col justify-center" href="/pages/rwarner31/ML-Project/midterm"> <div class="text-black text-2xl hover:opacity-80">Midterm Checkpoint</div> </a> <a href="/pages/rwarner31/ML-Project/proposal" class="flex flex-col justify-center"> <div class="text-black text-2xl hover:opacity-80">Project Proposal</div> </a> <a href="/pages/rwarner31/ML-Project/contributors" class="flex flex-col justify-center"> <div class="text-black text-2xl hover:opacity-80">Contributors</div> </a> </div> </div>  <div class="flex flex-col gap-12"> <div class="h-[60vh] w-full flex flex-col justify-center bg-gradient-to-r from-indigo-500 via-sky-500 to-emerald-500"> <div class="text-white text-8xl font-semibold px-[15%]">Final Report</div> </div> <div class=" px-[15%] flex flex-col gap-8"> <div class="flex flex-col gap-4 divide-y-4"> <div class="text-6xl font-semibold">Introduction and Background</div> <div class="flex flex-col gap-4"> <div class="text-2xl">
The objective of our project is to whether the source of a photo is a human photographer or an AI agent.
</div> <div class="text-2xl">
Modern research on the topic is extensive. Many share the same concerns regarding AI generated content, and the danger if we are unable to distinguish between what is generated by an AI and what is not. Baraheem [1] discuss whether AIs themselves are truly able to make this distinction, although they claim a suspiciously high accuracy of 100% on their dataset. Epstein [3] presents a similar discussion, highlighting the challenges associated with this sort of classification when AI development is so rapid, extending their analysis to a simulated release cadence. GÃ¶ring et. al [4] present a more high-level approach, discussing the appeal of AI images in the first place, an analysis that is slightly more qualitative but no less thought-provoking.
</div> <div class="text-2xl">
The dataset we have chosen is the <a class="font-bold hover:underline" target="_blank" href="https://ai.google.com/research/ConceptualCaptions/download">Google Conceptual Captions dataset [8]</a> and the <a class="font-bold hover:underline" target="_blank" href="https://huggingface.co/datasets/InfImagine/FakeImageDataset">Hugging Face InfImagine Fake Image dataset [6][7]</a>. Conceptual Captions is created by researchers at Google. It contains ~2 million real images and provides a conceptual caption for each. The goal of the research is to automate and legibly caption images. It obtains the images from webpages and the labels from Google Cloud Vision API. Lastly, we used the <a class="font-bold hover:underline" target="_blank" href="https://www.kaggle.com/datasets/birdy654/cifake-real-and-ai-generated-synthetic-images/data">CIFAKE dataset[2][5]</a> which contains 100,000 real and fake training images alongside 20,000 real and fake testing images.
</div> </div> </div> <div class="flex flex-col gap-4 divide-y-4"> <div class="text-6xl font-semibold">Problem Definition</div> <div class="flex flex-col gap-4"> <div class="text-2xl">
Artificial intelligence is one of the hottest topics of the modern world. Many students, ourselves included, reply on services such as ChatGPT or GitHub Copilot to augment our capabilities as students, employees, and learners. When used appropriately, these tools are incredibly powerful; however, misuse is not just possible but widespread.
</div> <div class="text-2xl ">
We are rapidly approaching a point where AI generated content is indistinguishable from that created by a human. In the photography domain, this is particularly concerning. We think that photos are representative of human experience and emotions, in a way that is worth preserving. With this project we hope to determine, to a reasonable degree of accuracy, the source of an image, AI or human. The goal is not to inhibit or even look down on the incredible potential of AI tools, but to ensure that the human experience is not lost in the process.
</div> </div> </div> <div class="flex flex-col gap-4 divide-y-4"> <div class="text-6xl font-semibold">Methods</div> <div class="flex flex-col gap-4"> <div class="flex flex-col gap-2"> <div class="text-4xl font-semibold">Preprocessing</div> <div class="text-2xl">
We were able to implement all preprocessing methods discussed in the proposal while implementing our model, as discussed below:
<ul class="list-disc px-8"> <li><strong>Image Augmentation</strong> Artificially increases the size of the dataset by applying random transformations to the images, allowing the model to learn from a wider variety of images without needing to find a larger dataset. Also due to storage limitations, we have to stream the data, so the ability to augment the dataset in memory as we stream images is incredibly helpful, and should increase the overall accuracy of the classifier.</li> <li><strong>Grayscale Conversion</strong> This has proved to be incredibly helpful in our implementation, significantly reducing the number of features in our dataset by collapsing RGB images to flat arrays.</li> <li><strong>Image Standardization</strong> Generally a good best practice, we utilized Scikit-Learn's StandardScaler utility to scale our data to 0-mean and unit variance. </li> </ul> </div> </div> <div class="flex flex-col gap-2"> <div class="text-4xl font-semibold">Modeling: </div> <div class="text-2xl"> <ul class="list-disc px-8"> <li><strong>Convolutional Neural Networks</strong> are widely used in image classification and segmentation, which may prove helpful if AI generators leave unique artifacts a human may not.</li> <li><strong>Generative Adversarial Networks</strong> may also be another option and might not require an AI generated dataset, as the discriminator in a GAN is trained to distinguish between the input and generated dataset.</li> <li><strong>Support Vector Machines</strong> good at working with high-dimensional data such as images and are generally less prone to overfitting, helpful if we want our model to be truly general.</li> </ul> </div> </div> </div> </div> <div class="flex flex-col gap-4 divide-y-4"> <div class="text-6xl font-semibold">Results and Discussion</div> <div class="text-2xl">Our quantitative scoring metrics are:<strong> accuracy, precision, and recall.</strong> Accuracy of at least 0.9 in distinguishing between AI and human generated images. Accuracy is a good general metric to quantify model performance as it calculates correct predicitions out of total ground truth (labels). Precision of at least 0.9 to show how good the ground truths are represented via true positives out of total ground truth. Recall of at least 0.8 to measure completeness and how well ground truth is recovered via true positive out of actual results; we care that the model is able to correctly identify AI generated images (true positive), but a little less if it mislabels a human generated image as AI (false negative). Project success is additionally quantified by a <strong>f1-score</strong> of at least 0.8 as it measures precision and completeness.<div class="text-2xl"></div> <br><br> <div class="text-3xl font-semibold">SVM Metrics</div> <img src="/pages/rwarner31/ML-Project/assets/ProjResults.png" alt="Confusion Matrix" class="w-fuk=ll h-auto"> <div class="text-2xl">We implemented SVM and visualized our results via the classification report, confusion matrix, and accuracy scores above.
<div> <br><br> <div class="text-3xl font-semibold">Confusion Matrix for a 1k Testing Set</div> <div class="w-full flex justify-center"> <div class="p-4 w-1/2"> <img src="/pages/rwarner31/ML-Project/assets/CONFUSION_SVM.png" alt="Confusion Matrix" class="w-full h-auto"> </div> </div> </div> <div class="text-2xl">We implemented SVM and visualized our results in the confusion matrix above. Our quantitative scoring metrics are:<strong> accuracy, precision, and recall.</strong> Accuracy of at least 0.9 in distinguishing between AI and human generated images. Accuracy is a good general metric to quantify model performance as it calculates correct predicitions out of total ground truth (labels). Precision of at least 0.9 to show how good the ground truths are represented via true positives out of total ground truth. Recall of at least 0.8 to measure completeness and how well ground truth is recovered via true positive out of actual results; we care that the model is able to correctly identify AI generated images (true positive), but a little less if it mislabels a human generated image as AI (false negative). Project success is additionally quantified by a <strong>f1-score</strong> of at least 0.8 as it measures precision and completeness.<div class="text-2xl"></div> <br>From our visualization (confusion matrix), all of our quantitative scoring metrics and inherent goals were not met. Our model has an accuracy of <strong>0.553</strong>, with an f1 score in classifying real images of <strong>0.55</strong>, and a score of <strong>0.56</strong> in classifying fake images. Compared to the "success" metrics above, our model is well short of the mark. Our predictions were not precise, did not recover the ground truth well, and was not accurate to our standards; it was unable to effectively identify AI generated images from real images. Our model performed poorly because our sample size were small and SVM needs a large sample to train well. That said, it is possible that what we are seeing is to be expected of a support vector machine. Since we are using image data, our dataset is quite large, and even on ICE, unable to be downloaded in its entirety. Because of this, we have been forced to adapt our approach and stream the dataset, which greatly affects the runtime and performance of our model. To prevent loading everything into memory, a SGD classifier, which exhibits behavior incredibly similar to that of an SVM was used since it allows us to partially fit the model with smaller batches of data. With these necessities in mind, we have only trained the model so far on 20k records, a relatively small dataset on the scale of what is required for an accurate model. On the other hand, since SVMs are generally fairly resiliant against overfitting, the model may simply be undertrained, again, given the size of the dataset.
<br><br> <div> <div class="text-3xl font-semibold">CNN Metrics</div> <div class="w-full flex justify-center"> <div class="p-4 w-9/10"> <img src="/pages/rwarner31/ML-Project/assets/cnnPlots.png" alt="CNN metrics" class="w-full h-auto" style="max-height: 3000px; max-width: 5000px;"> </div> </div>
Our CNN metrics inform us about the both the training and validation data's loss, accuracy, precision, and recall over about 14 epochs. The loss tells us about the difference in our predicted outputs versus the actual values. The lower the loss value is, the closer our predictions are to the actual target value. Thus as our model becomes more accurate, the loss decreases. Viewing the accuracy plot to the right of the loss function, the accuracy changes accordingly. Since our CNN model performed with a high accuracy, the loss decreases. The validation set follows the trend of the training set. The precision represents the degree of purity the model displays. In our visualization, the mode's precision gradually increases as the clusters better represent the ground truth.
<br><br> <div class="text-3xl font-semibold">CNN Confusion Matrix</div> <div class="w-full flex justify-center"> <div class="p-4 w-9/10"> <img src="/pages/rwarner31/ML-Project/assets/confusioncnn.png" alt="CNN metrics" class="w-fuk=ll h-auto" style="max-height: 1000px; max-width: 600px;"> </div> </div> <div class="w-full flex justify-center"> <div class="p-4 w-9/10"> <img src="/pages/rwarner31/ML-Project/assets/cnnStats.png" alt="CNN metrics" class="w-full h-auto" style="max-height: 5000px; max-width: 5000px;"> </div> </div>
The confusion matrix displays a high true positive and high true negative. Therefore, analyzing the confusion matrix we can see that the precision of our model is very high, with there only being a minimal amount of false positives. Additionally, the recall is high because there isn't a significant amount of false negatives.
<br><br>
Our CNN model achieved an accuracy of 92%, which meets our quantitative scoring goal. Moreover, our model scored a recall of 0.92 and model ROC-AUC of 0.917. Thus, the model overall reaches our scoring metric goals that we initially set.
<br><br>
SVMs are good for high-dimensional data and resistant to overfitting. However, when the target classes overlap, as is our case with generated content, performance can suffer. They are also computationally expensive which limits our ability to train them on a large database. For CNNs, they learn hierarchical features from images and are powerful for image classification; its architecture allows the model to learn small features in an image, assuming that AI generators leave unique artifacts that humans do not. However, CNNs need to train on large labeled datasets and are computational expensive. GANs are also good as they can create new data based on input dataset and can improve detection performance, however, it is hard to train, very sensitive to hyperparameters, and computationally expensive. Overall, CNNs are better at the classification task due to its nature and being trained on a decently sized dataset. Although it is computationally expensive, we were able to use PACE to speed up training. Our metrics tell us that we were not able to properly train our SVM and GAN models while CNN was a success. Our SVM model performed poorly because the dataset used for it was not big enough. Additionally, we were not able to tune the hyperparameters well for the GAN model, making it unstable for its role. However, CNN performed well as we had enough layers and trained it on a small but still appropriately sized dataset (100k training images); this is expected since CNN is known for its performance in image classification. In summary, CNN performed the best, achieving our success metrics, SVM performed moderately but effective features were not engineered, and GAN performed the worst because we did not fine tune the parameters enough. Overall, our CNN model performed extremely well for this data because of the binary classification and the nature of CNN. Using convolutional layers, flattening, and pooling, the model can compress a fully connected network.
</div> <br><br> <div> <div class="text-3xl font-semibold">GAN Metrics</div> <div class="inline-block justify-center"> <div class="p-4 w-100"> <img src="/pages/rwarner31/ML-Project/assets/gan1.png" alt="GAN metrics" class="w-full h-auto" style="max-height: 3000px; max-width: 5000px;"> </div> </div> <div class="inline-block justify-center"> <div class="p-4 w-100"> <img src="/pages/rwarner31/ML-Project/assets/gan2.png" alt="GAN metrics" class="w-full h-auto" style="max-height: 3000px; max-width: 5000px;"> </div> </div> <div class="inline-block justify-center"> <div class="p-4 w-100"> <img src="/pages/rwarner31/ML-Project/assets/gan3.png" alt="GAN metrics" class="w-full h-auto" style="max-height: 3000px; max-width: 5000px;"> </div> </div> <br><br>
GAN uses a generator and a discriminator at the same time. The discriminator's purpose is to differentiate the real and fake images, and it can adapt to the generator's increasingly more realistic images. We implemented a conditional GAN from TorchGan and the classification was carried out with a DCGAN discriminator. However, our GAN model performed very poorly, having poor precision and recall, and is wrong more times than right, as observed from the confusion matrix with the false positives and false negatives.
<br><br> <div class="text-3xl font-semibold">GAN Confusion Matrix</div> <div class="w-full flex justify-center"> <div class="p-4 w-1/2"> <img src="/pages/rwarner31/ML-Project/assets/CONFUSION_GAN.png" alt="Confusion Matrix" class="w-full h-auto"> </div> </div> </div> </div></div> <br><br> <div class="flex flex-col gap-4 divide-y-4"> <div class="text-6xl font-semibold">Next Steps</div> <div class="flex flex-col gap-4">
We can train CNN on a larger dataset and on more epochs to see its maximum potential. In regards to SVM, we can review the features we are using, use a bigger dataset, and use techniques like PCA to reduce the number of features but still keep variance and therefore the informative features. Also, for the case of GAN, we need to keep tuning the architecture, implement training techniques like gradient penalty to stabilize the training, and adjust the parameters of learning rate, training ratio, etc.
<div class="text-2xl"></div> </div> </div> <div class="flex flex-col gap-4 divide-y-4"> <div class="text-6xl font-semibold">References</div> <div class="text-2xl flex flex-col gap-2"> 
  <div data-csl-entry-id="jimaging9100199" class="csl-entry">Baraheem, S. S., &#38; Nguyen, T. V. (2023). AI vs. AI: Can AI Detect AI-Generated Images? <i>Journal of Imaging</i>, <i>9</i>(10), 199. https://doi.org/10.3390/jimaging9100199</div>
  <div data-csl-entry-id="cifake" class="csl-entry">Bird, J., &#38; Lotfi, A. (2024). <i>CIFAKE: Image Classification and Explainable Identification of AI-Generated Synthetic Images</i>.</div>
  <div data-csl-entry-id="Epstein_2023_ICCV" class="csl-entry">Epstein, D. C., Jain, I., Wang, O., &#38; Zhang, R. (2023). Online Detection of AI-Generated Images. <i>Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops</i>, 382â392.</div>
  <div data-csl-entry-id="10103686" class="csl-entry">GÃ¶ring, S., Ramachandra Rao, R. R., Merten, R., &#38; Raake, A. (2023). Analysis of Appeal for Realistic AI-Generated Photos. <i>IEEE Access</i>, <i>11</i>, 38999â39012. https://doi.org/10.1109/ACCESS.2023.3267968</div>
  <div data-csl-entry-id="krizhevsky2009learning" class="csl-entry">Krizhevsky, A., &#38; Hinton, G. (2098). <i>Learning multiple layers of features from tiny images</i>.</div>
  <div data-csl-entry-id="lu2023seeing" class="csl-entry">Lu, Z., Huang, D., Bai, L., Qu, J., Wu, C., Liu, X., &#38; Ouyang, W. (2023). <i>Seeing is not always believing: Benchmarking Human and Model Perception of AI-Generated Images</i>. arXiv.</div>
  <div data-csl-entry-id="sentry-image-leaderboard" class="csl-entry">Lu, Z., Huang, D., Zhang, C., Wu, C., Liu, X., Bai, L., &#38; Ouyang, W. (2023). <i>Sentry-Image Leaderboard</i>.</div>
  <div data-csl-entry-id="google" class="csl-entry">Sharma, P., Ding, N., Goodman, S., &#38; Soricut, R. (2018). <i>Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning</i>.</div>
 </div> </div> <br><br> <div class="flex flex-col gap-4 divide-y-4"> <div class="text-6xl font-semibold">Gantt Chart</div> <img src="/pages/rwarner31/ML-Project/assets/gantt3.png" alt="Gantt" class="w-fuk=ll h-auto"> </div> </div> </div> </div></div> </div> <div class="h-48&quot;"> <div class="w-full h-32 flex flex-col justify-center align-middle"> <div class="font-semibold text-2xl self-center h-min"> Made with â¤ in Atlanta, GA </div> </div> </div> </body></html>