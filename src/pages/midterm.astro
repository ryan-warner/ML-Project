---
import Cite from 'citation-js' 
import references from '../assets/references.json'

const cite = new Cite(references)

let htmlRefs = cite.format('bibliography', {
  format: 'html',
  template: 'ieee',
  lang: 'en-US',
})

htmlRefs = htmlRefs.slice(26, htmlRefs.length - 6)
import Layout from '../layouts/Layout.astro'
---
<Layout>
    <div class="flex flex-col gap-12">
        <div class="h-[60vh] w-full flex flex-col justify-center bg-gradient-to-r from-indigo-500 via-sky-500 to-emerald-500">
            <div class="text-white text-8xl font-semibold px-[15%]">Midterm</div>
        </div>
        <div class=" px-[15%] flex flex-col gap-8">
            <div class="flex flex-col gap-4 divide-y-4">
                <div class="text-6xl font-semibold">Introduction and Background</div>
                <div class="flex flex-col gap-4">
                    <div class="text-2xl">
                        The objective of our project is to whether the source of a photo is a human photographer or an AI agent. 
                    </div>
                    <div class="text-2xl">
                        Modern research on the topic is extensive. Many share the same concerns regarding AI generated content, and the danger if we are unable to distinguish between what is generated by an AI and what is not. Baraheem [1] discuss whether AIs themselves are truly able to make this distinction, although they claim a suspiciously high accuracy of 100% on their dataset. Epstein [2] presents a similar discussion, highlighting the challenges associated with this sort of classification when AI development is so rapid, extending their analysis to a simulated release cadence. GÃ¶ring et. al [3] present a more high-level approach, discussing the appeal of AI images in the first place, an analysis that is slightly more qualitative but no less thought-provoking.
                    </div>
                    <div class="text-2xl">
                        The dataset we have chosen is the <a class="font-bold hover:underline" target="_blank" href="https://ai.google.com/research/ConceptualCaptions/download">Google Conceptual Captions dataset [6]</a> and the <a class="font-bold hover:underline" target="_blank" href="https://huggingface.co/datasets/InfImagine/FakeImageDataset">Hugging Face InfImagine Fake Image dataset [4][5]</a>. Conceptual Captions is created by researchers at Google. It contains ~2 million real images and provides a conceptual caption for each. The goal of the research is to automate and legibly caption images. It obtains the images from webpages and the labels from Google Cloud Vision API.
                    </div>
                </div>
            </div>
            <div class="flex flex-col gap-4 divide-y-4">
                <div class="text-6xl font-semibold">Problem Definition</div>
                <div class="flex flex-col gap-4">
                    <div class="text-2xl">
                        Artificial intelligence is one of the hottest topics of the modern world. Many students, ourselves included, reply on services such as ChatGPT or GitHub Copilot to augment our capabilities as students, employees, and learners. When used appropriately, these tools are incredibly powerful; however, misuse is not just possible but widespread.
                    </div>
                    <div class="text-2xl ">
                        We are rapidly approaching a point where AI generated content is indistinguishable from that created by a human. In the photography domain, this is particularly concerning. We think that photos are representative of human experience and emotions, in a way that is worth preserving. With this project we hope to determine, to a reasonable degree of accuracy, the source of an image, AI or human. The goal is not to inhibit or even look down on the incredible potential of AI tools, but to ensure that the human experience is not lost in the process.
                    </div>
                </div>
            </div>
            <div class="flex flex-col gap-4 divide-y-4">
                <div class="text-6xl font-semibold">Methods</div>
                <div class="flex flex-col gap-4">
                    <div class="flex flex-col gap-2">
                        <div class="text-4xl font-semibold">Preprocessing</div>
                        <div class="text-2xl">
                            We chose image augmentation it suits the dilemma we faced while implementing our model, as discussed below: 
                            <ul class="list-disc px-8">
                                <li><strong>Image Augmentation</strong> artificially increases the size of the dataset by applying random transformations to the images, allowing the model to learn from a wider variety of images without needing to find a larger dataset. We currently face the dilemma of 20k samples. Also due to storage limitations, we have to stream the data, which takes longer than directly through local</li>
                            </ul>
                        </div>
                    </div>
                    <div class="flex flex-col gap-2">
                        <div class="text-4xl font-semibold">Modeling: </div>
                        <div class="text-2xl">
                            We implemented SVM for the classification model because we thought it is well-suited for bias-variance tradeoff which comes when the model is complicated, such is the case for image classification:
                            <ul class="list-disc px-8">
                                <li><strong>Support Vector Machines</strong> good at working with high-dimensional data such as images and are generally less prone to overfitting, helpful if we want our model to be truly general.</li>
                        </div>
                    </div>
                </div>
            </div>

            <div class="flex flex-col gap-4 divide-y-4">
                <div class="text-6xl font-semibold">Results and Discussion</div>
                <div class="text-3xl font-semibold">Confusion Matrix</div>
                <img src="/pages/rwarner31/ML-Project/assets/ProjResults.png" alt="Confusion Matrix" class="w-fuk=ll h-auto" />
                <div class="text-2xl">We implemented SVM and visualized our results via the classification report, confusion matrix, and accuracy scores above. Our quantitative scoring metrics are:<strong> accuracy, precision, and recall.</strong> Accuracy of at least 0.9 in distinguishing between AI and human generated images. Accuracy is a good general metric to quantify model performance as it calculates correct predicitions out of total ground truth (labels). Precision of at least 0.9 to show how good the ground truths are represented via true positives out of total ground truth. Recall of at least 0.8 to measure completeness and how well ground truth is recovered via true positive out of actual results; we care that the model is able to correctly identify AI generated images (true positive), but a little less if it mislabels a human generated image as AI (false negative). Project success is additionally quantified by a <strong>f1-score</strong> score of at least 0.8 as it measures precision and completeness.<div class="text-2xl">From our visualization, all of our quantitative scoring metrics and inherent goals were not met. Our predictions were not precise, did not recover the ground truth well, and was not accurate to our standards; it was unable to effectively identify AI generated images from real images. Our model performed poorly because our sample size were small and SVM needs a large sample to train well. Our next steps are using PACE environment, which has more computation power fixing GPU limitations, to run more iterations and on more samples to fix the underfitting problem. 
</div></div>

            <div class="flex flex-col gap-4 divide-y-4">
                <div class="text-6xl font-semibold">References</div>
                <div class="text-2xl flex flex-col gap-2">
                    <Fragment set:html={htmlRefs} />
                </div>
            </div>

            <div class="flex flex-col gap-4 divide-y-4">
                <div class="text-6xl font-semibold">Gantt Chart</div>
                <img src="/pages/rwarner31/ML-Project/assets/Gannt2.png" alt="Gantt" class="w-fuk=ll h-auto" />
            </div>
        </div>
    </div>
</Layout>
