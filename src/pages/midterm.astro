---
import Cite from 'citation-js' 
import references from '../assets/references.json'

const cite = new Cite(references)

let htmlRefs = cite.format('bibliography', {
  format: 'html',
  template: 'ieee',
  lang: 'en-US',
})

htmlRefs = htmlRefs.slice(26, htmlRefs.length - 6)
import Layout from '../layouts/Layout.astro'
---
<Layout>
    <div class="flex flex-col gap-12">
        <div class="h-[60vh] w-full flex flex-col justify-center bg-gradient-to-r from-indigo-500 via-sky-500 to-emerald-500">
            <div class="text-white text-8xl font-semibold px-[15%]">Midterm</div>
        </div>
        <div class=" px-[15%] flex flex-col gap-8">
            <div class="flex flex-col gap-4 divide-y-4">
                <div class="text-6xl font-semibold">Introduction and Background</div>
                <div class="flex flex-col gap-4">
                    <div class="text-2xl">
                        The objective of our project is to whether the source of a photo is a human photographer or an AI agent. 
                    </div>
                    <div class="text-2xl">
                        Modern research on the topic is extensive. Many share the same concerns regarding AI generated content, and the danger if we are unable to distinguish between what is generated by an AI and what is not. Baraheem [1] discuss whether AIs themselves are truly able to make this distinction, although they claim a suspiciously high accuracy of 100% on their dataset. Epstein [2] presents a similar discussion, highlighting the challenges associated with this sort of classification when AI development is so rapid, extending their analysis to a simulated release cadence. GÃ¶ring et. al [3] present a more high-level approach, discussing the appeal of AI images in the first place, an analysis that is slightly more qualitative but no less thought-provoking.
                    </div>
                    <div class="text-2xl">
                        The dataset we have chosen is the <a class="font-bold hover:underline" target="_blank" href="https://ai.google.com/research/ConceptualCaptions/download">Google Conceptual Captions dataset [6]</a> and the <a class="font-bold hover:underline" target="_blank" href="https://huggingface.co/datasets/InfImagine/FakeImageDataset">Hugging Face InfImagine Fake Image dataset [4][5]</a>. Conceptual Captions is created by researchers at Google. It contains ~2 million real images and provides a conceptual caption for each. The goal of the research is to automate and legibly caption images. It obtains the images from webpages and the labels from Google Cloud Vision API.
                    </div>
                </div>
            </div>
            <div class="flex flex-col gap-4 divide-y-4">
                <div class="text-6xl font-semibold">Problem Definition</div>
                <div class="flex flex-col gap-4">
                    <div class="text-2xl">
                        Artificial intelligence is one of the hottest topics of the modern world. Many students, ourselves included, reply on services such as ChatGPT or GitHub Copilot to augment our capabilities as students, employees, and learners. When used appropriately, these tools are incredibly powerful; however, misuse is not just possible but widespread.
                    </div>
                    <div class="text-2xl ">
                        We are rapidly approaching a point where AI generated content is indistinguishable from that created by a human. In the photography domain, this is particularly concerning. We think that photos are representative of human experience and emotions, in a way that is worth preserving. With this project we hope to determine, to a reasonable degree of accuracy, the source of an image, AI or human. The goal is not to inhibit or even look down on the incredible potential of AI tools, but to ensure that the human experience is not lost in the process.
                    </div>
                </div>
            </div>
            <div class="flex flex-col gap-4 divide-y-4">
                <div class="text-6xl font-semibold">Methods</div>
                <div class="flex flex-col gap-4">
                    <div class="flex flex-col gap-2">
                        <div class="text-4xl font-semibold">Preprocessing</div>
                        <div class="text-2xl">
                            We were able to implement all preprocessing methods discussed in the proposal while implementing our model, as discussed below: 
                            <ul class="list-disc px-8">
                                <li><strong>Image Augmentation</strong> Artificially increases the size of the dataset by applying random transformations to the images, allowing the model to learn from a wider variety of images without needing to find a larger dataset. Also due to storage limitations, we have to stream the data, so the ability to augment the dataset in memory as we stream images is incredibly helpful, and should increase the overall accuracy of the classifier.</li>
                                <li><strong>Grayscale Conversion</strong> This has proved to be incredibly helpful in our implementation, significantly reducing the number of features in our dataset by collapsing RGB images to flat arrays.</li>
                                <li><strong>Image Standardization</strong> Generally a good best practice, we utilized Scikit-Learn's StandardScaler utility to scale our data to 0-mean and unit variance. </li>
                            </ul>
                        </div>
                    </div>
                    <div class="flex flex-col gap-2">
                        <div class="text-4xl font-semibold">Modeling: </div>
                        <div class="text-2xl">
                            We implemented a SVM for the initial classification model because we thought it is well-suited for bias-variance tradeoff which comes when the model is complicated, such is the case for image classification:
                            <ul class="list-disc px-8">
                                <li><strong>Support Vector Machines</strong> good at working with high-dimensional data such as images and are generally less prone to overfitting, helpful if we want our model to be truly general.</li>
                        </div>
                    </div>
                </div>
            </div>

            <div class="flex flex-col gap-4 divide-y-4">
                <div class="text-6xl font-semibold">Results and Discussion</div>
                <div>
                    <div class="text-3xl font-semibold">Confusion Matrix for a 1k Testing Set</div>
                    <!-- <img src="/pages/rwarner31/ML-Project/assets/ProjResults.png" alt="Confusion Matrix" class="w-fuk=ll h-auto" /> -->
                    <div class="w-full flex justify-center">
                        <div class="p-4 w-1/2">
                            <img src="/pages/rwarner31/ML-Project/assets/confusion.png" alt="Confusion Matrix" class="w-full h-auto" />
                            <!-- <div class="grid grid-cols-4 grid-flow-row">
                                <div class="col-span-1"></div>
                                <div class="col-span-1"></div>
                                <div class="col-span-2 flex justify-center text-2xl">Actual</div>
                                <div class="col-span-1"></div>
                                <div class="col-span-1"></div>
                                <div class="text-2xl flex justify-center">Positive</div>
                                <div class="text-2xl flex justify-center">Negative</div>
                                <div style={{"writing-mode": 'vertical-rl'}} class="row-span-3 text-2xl rotate-180">Predicted</div>
                                <div class="text-2xl">Positive</div>
                                <div class="border-t-4 border-l-4 border-r-2 border-b-2 border-black flex justify-center text-xl">560</div>
                                <div class="border-t-4 border-r-4 border-l-2 border-b-2 border-black flex justify-center text-xl">440</div>
                                <div class="text-2xl">Negative</div>
                                <div class="border-b-4 border-l-4 border-r-2 border-t-2 border-black flex justify-center text-xl">454</div>
                                <div class="border-b-4 border-r-4 border-l-2 border-t-2 border-black flex justify-center text-xl">546</div>
                            </div> -->
                        </div>
                    </div>
                </div>
                <div class="text-2xl">We implemented SVM and visualized our results via the confusion matrix above. Our quantitative scoring metrics are:<strong> accuracy, precision, and recall.</strong> Accuracy of at least 0.9 in distinguishing between AI and human generated images. Accuracy is a good general metric to quantify model performance as it calculates correct predicitions out of total ground truth (labels). Precision of at least 0.9 to show how good the ground truths are represented via true positives out of total ground truth. Recall of at least 0.8 to measure completeness and how well ground truth is recovered via true positive out of actual results; we care that the model is able to correctly identify AI generated images (true positive), but a little less if it mislabels a human generated image as AI (false negative). Project success is additionally quantified by a <strong>f1-score</strong> of at least 0.8 as it measures precision and completeness.<div class="text-2xl"></div>
            
                </br>From our visualization (confusion matrix), all of our quantitative scoring metrics and inherent goals were not met. Our model has an accuracy of <strong>0.553</strong>, with an f1 score in classifying real images of <strong>0.55</strong>, and a score of <strong>0.56</strong> in classifying fake images. Compared to the scores statistics above, our model is well short of the mark. Our predictions were not precise, did not recover the ground truth well, and was not accurate to our standards; it was unable to effectively identify AI generated images from real images. Our model performed poorly because our sample size were small and SVM needs a large sample to train well. That said, it is possible that what we are seeing is to be expected of a support vector machine. Since we are using image data, our dataset is quite large, and even on ICE, unable to be downloaded in its entirety. Because of this, we have been forced to adapt our approach and stream the dataset, which greatly affects the runtime and performance of our model. To prevent loading everything into memory, a SGD classifier, which exhibits behavior incredibly similar to that of an SVM was used since it allows us to partially fit the model with smaller batches of data. With these necessiies in mind, we have only trained the model so far on 20k records, a relatively small dataset on the scale of what is required for an accurate model. On the other hand, since SVMs are generally fairly resiliant against overfitting, the model may simply be undertrained, again, given the size of the dataset. Other models, such as the CNN, which we are currently in the process of implementing, do not exhibit the same overfitting tendencies, so we hope to see improved resuts, even with this smaller dataset. 

            </br></br> The CNN and GAN implementations are loosely complete. What remains is to connect these to the dataset classes developed on the SVM implementation and train them on the ICE cluster. We are working to parallelize the data streaming process to better utilize the available computational resources and allow us to train these models on a much larger dataset. 
</div></div>

            <div class="flex flex-col gap-4 divide-y-4">
                <div class="text-6xl font-semibold">References</div>
                <div class="text-2xl flex flex-col gap-2">
                    <Fragment set:html={htmlRefs} />
                </div>
            </div>

            <div class="flex flex-col gap-4 divide-y-4">
                <div class="text-6xl font-semibold">Gantt Chart</div>
                <img src="/pages/rwarner31/ML-Project/assets/Gannt2.png" alt="Gantt" class="w-fuk=ll h-auto" />
            </div>
        </div>
    </div>
</Layout>
