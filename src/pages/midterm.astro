---
import Cite from 'citation-js' 
import references from '../assets/references.json'

const cite = new Cite(references)

let htmlRefs = cite.format('bibliography', {
  format: 'html',
  template: 'ieee',
  lang: 'en-US',
})

htmlRefs = htmlRefs.slice(26, htmlRefs.length - 6)
import Layout from '../layouts/Layout.astro'
---
<Layout>
    <div class="flex flex-col gap-12">
        <div class="h-[60vh] w-full flex flex-col justify-center bg-gradient-to-r from-indigo-500 via-sky-500 to-emerald-500">
            <div class="text-white text-8xl font-semibold px-[15%]">Midterm</div>
        </div>
        <div class=" px-[15%] flex flex-col gap-8">
            <div class="flex flex-col gap-4 divide-y-4">
                <div class="text-6xl font-semibold">Introduction and Background</div>
                <div class="flex flex-col gap-4">
                    <div class="text-2xl">
                        The objective of our project is to whether the source of a photo is a human photographer or an AI agent. 
                    </div>
                    <div class="text-2xl">
                        Modern research on the topic is extensive. Many share the same concerns regarding AI generated content, and the danger if we are unable to distinguish between what is generated by an AI and what is not. Baraheem [1] discuss whether AIs themselves are truly able to make this distinction, although they claim a suspiciously high accuracy of 100% on their dataset. Epstein [2] presents a similar discussion, highlighting the challenges associated with this sort of classification when AI development is so rapid, extending their analysis to a simulated release cadence. GÃ¶ring et. al [3] present a more high-level approach, discussing the appeal of AI images in the first place, an analysis that is slightly more qualitative but no less thought-provoking.
                    </div>
                    <div class="text-2xl">
                        The dataset we have chosen is the <a class="font-bold hover:underline" target="_blank" href="https://ai.google.com/research/ConceptualCaptions/download">Google Conceptual Captions dataset [6]</a> and the <a class="font-bold hover:underline" target="_blank" href="https://huggingface.co/datasets/InfImagine/FakeImageDataset">Hugging Face InfImagine Fake Image dataset [4][5]</a>. Conceptual Captions is created by researchers at Google. It contains ~2 million real images and provides a conceptual caption for each. The goal of the research is to automate and legibly caption images. It obtains the images from webpages and the labels from Google Cloud Vision API.
                    </div>
                </div>
            </div>
            <div class="flex flex-col gap-4 divide-y-4">
                <div class="text-6xl font-semibold">Problem Definition</div>
                <div class="flex flex-col gap-4">
                    <div class="text-2xl">
                        Artificial intelligence is one of the hottest topics of the modern world. Many students, ourselves included, reply on services such as ChatGPT or GitHub Copilot to augment our capabilities as students, employees, and learners. When used appropriately, these tools are incredibly powerful; however, misuse is not just possible but widespread.
                    </div>
                    <div class="text-2xl ">
                        We are rapidly approaching a point where AI generated content is indistinguishable from that created by a human. In the photography domain, this is particularly concerning. We think that photos are representative of human experience and emotions, in a way that is worth preserving. With this project we hope to determine, to a reasonable degree of accuracy, the source of an image, AI or human. The goal is not to inhibit or even look down on the incredible potential of AI tools, but to ensure that the human experience is not lost in the process.
                    </div>
                </div>
            </div>
            <div class="flex flex-col gap-4 divide-y-4">
                <div class="text-6xl font-semibold">Methods</div>
                <div class="flex flex-col gap-4">
                    <div class="flex flex-col gap-2">
                        <div class="text-4xl font-semibold">Preprocessing</div>
                        <div class="text-2xl">
                            The following methods, typical of image classification research, have been chosen because they offer the following benefits:
                            <ul class="list-disc px-8">
                                <li><strong>Greyscale Conversion</strong> reduces the dimensionality of the dataset, making it computationally cheaper to work on.</li>
                                <li><strong>Image Standardization</strong> scales pixel values of an image to a mean of 0 and a standard deviation of 1, ensuring that the model is not biased towards any particular pixel value.</li>
                                <li><strong>Image Augmentation</strong> artificially increases the size of the dataset by applying random transformations to the images, allowing the model to learn from a wider variety of images without needing to find a larger dataset.</li>
                            </ul>
                        </div>
                    </div>
                    <div class="flex flex-col gap-2">
                        <div class="text-4xl font-semibold">Modeling: </div>
                        <div class="text-2xl">
                            The folliwing methods have been identified as potential candidates for the classification model:
                            <ul class="list-disc px-8">
                                <li><strong>Convolutional Neural Networks</strong> are widely used in image classification and segmentation, which may prove helpful if AI generators leave unique artifacts a human may not.</li>
                                <li><strong>Support Vector Machines</strong> are another candidate, good at working with high-dimensional data such as images and are generally less prone to overfitting, helpful if we want our model to be truly general.</li>
                                <li><strong>Generative Adversarial Networks</strong> may also be another option and might not require an AI generated dataset, as the discriminator in a GAN is trained to distinguish between the input and generated dataset.</li> 
                        </div>
                    </div>
                </div>
            </div>

            <div class="flex flex-col gap-4 divide-y-4">
                <div class="text-6xl font-semibold">Results and Discussion</div>
                <div class="text-2xl">
                    For this project, we hope to achieve an <strong>accuracy</strong> of at least 90% in distinguishing between AI and human generated images. Accuracy is a good general metric to quantify model performance. Project success is additionally quantified by a <strong>ROC AUC</strong> score of at least 0.8, generally considered acceptable, and quantifies how much the model is able to distinguish between the two classes. Lastly, we'll consider <strong>recall</strong>, with success marked by a score of at least 0.8, as we generally care that the model is able to correctly identify AI generated images (true positive), but a little less if it mislabels a human generated image as AI (false negative).
                </div>
            </div>

            <div class="flex flex-col gap-4 divide-y-4">
                <div class="text-6xl font-semibold">References</div>
                <div class="text-2xl flex flex-col gap-2">
                    <Fragment set:html={htmlRefs} />
                </div>
            </div>

            <div class="flex flex-col gap-4 divide-y-4">
                <div class="text-6xl font-semibold">Gantt Chart</div>
                <img src="/pages/rwarner31/ML-Project/assets/gantt.png" alt="Gantt" class="w-fuk=ll h-auto" />
            </div>
        </div>
    </div>
</Layout>
